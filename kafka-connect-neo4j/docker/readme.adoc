
==== Configuration parameters
:environment: neo4j
:id: neo4j

You can set the following configuration values via Confluent Connect UI, or via REST endpoint

[cols="3*",subs="attributes",options="header"]
|===
|Field|Type|Description

|{environment}.server.uri|String|The Bolt URI (default bolt://localhost:7687)
|{environment}.authentication.type|enum[NONE, BASIC, KERBEROS]| The authentication type (default BASIC)
|{environment}.batch.size|Int|The max number of events processed by the Cypher query (default 1000)
|{environment}.batch.timeout.msecs|Long|The execution timeout for the cypher query (default 30000)
|{environment}.authentication.basic.username|String| The authentication username
|{environment}.authentication.basic.password|String| The authentication password
|{environment}.authentication.basic.realm|String| The authentication realm
|{environment}.authentication.kerberos.ticket|String| The Kerberos ticket
|{environment}.encryption.enabled|Boolean| If the encryption is enabled (default false)
|{environment}.encryption.trust.strategy|enum[TRUST_ALL_CERTIFICATES, TRUST_CUSTOM_CA_SIGNED_CERTIFICATES, TRUST_SYSTEM_CA_SIGNED_CERTIFICATES]| The Neo4j trust strategy (default TRUST_ALL_CERTIFICATES)
|{environment}.encryption.ca.certificate.path|String| The path of the certificate
|{environment}.connection.max.lifetime.msecs|Long| The max Neo4j connection lifetime (default 1 hour)
|{environment}.connection.acquisition.timeout.msecs|Long| The max Neo4j acquisition timeout (default 1 hour)
|{environment}.connection.liveness.check.timeout.msecs|Long| The max Neo4j liveness check timeout (default 1 hour)
|{environment}.connection.max.pool.size|Int| The max pool size (default 100)
|{environment}.load.balance.strategy|enum[ROUND_ROBIN, LEAST_CONNECTED]| The Neo4j load balance strategy (default LEAST_CONNECTED)
|{environment}.batch.parallelize|Boolean|(default true) While concurrent batch processing improves throughput, it might cause out-of-order handling of events.  Set to `false` if you need application of messages with strict ordering, e.g. for change-data-capture (CDC) events.
|===

==== Configuring the stack

Start the compose file

[source,bash]
----
docker-compose up -d
----

You can access your Neo4j instance under: http://localhost:7474, log in with `neo4j` as username and `connect` as password (see the docker-compose file to change it).

===== Plugin installation

You can choose your preferred way in order to install the plugin:

* *Build it locally*
+
--
Build the project by running the following command:

[source,bash]
----
mvn clean install
----

Create a directory `plugins` at the same level of the compose file and unzip the file `neo4j-kafka-connect-neo4j-<VERSION>.zip` inside it.
--

* *Download the zip from the Confluent Hub*

+
--
Please go to the Confluent Hub page of the plugin:

https://www.confluent.io/connector/kafka-connect-neo4j-sink/

And click to the **Download Connector** button.

Create a directory `plugins` at the same level of the compose file and unzip the file `neo4j-kafka-connect-neo4j-<VERSION>.zip` inside it.
--

* *Download and install the plugin via Confluent Hub client*
+
--
If you are using the provided compose file you can easily install the plugin by using the Confluent Hub.

Once the compose file is up and running you can install the plugin by executing the following command:

[source,bash]
----
docker exec -it connect confluent-hub install neo4j/kafka-connect-neo4j:<version>
----

When the installation will ask:

[source,bash]
----
The component can be installed in any of the following Confluent Platform installations:
----

Please prefer the solution `(where this tool is installed)` and then go ahead with the default options.

At the end of the process the plugin is automatically installed.
--

==== Create the Sink Instance

To create the Sink instance and configure your preferred ingestion strategy, you can follow instructions described
into <<kafka-connect-sink-instance, Create the Sink Instance>> and <<kafka-connect-sink-strategies, Sink Ingestion Strategies>>
sections.

===== Use the Kafka Connect Datagen

In order to generate a sample dataset you can use Kafka Connect Datagen as explained in <<neo4j_confluent, `Example with Kafka Connect Datagen`>> section.

[NOTE]
Before start using the data generator please create indexes in Neo4j (in order to speed-up the import process)

