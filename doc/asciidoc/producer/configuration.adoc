.neo4j.conf
----
kafka.zookeeper.connect=localhost:2181
kafka.bootstrap.servers=localhost:9092
kafka.acks=1
kafka.retries=2
kafka.batch.size=16384
kafka.buffer.memory=33554432
kafka.reindex.batch.size=1000
kafka.session.timeout.ms=15000
kafka.connection.timeout.ms=10000
kafka.replication=1
kafka.linger.ms=1
kafka.transactional.id=
kafka.topic.discovery.polling.interval=300000
kafka.streams.log.compaction.strategy=delete

streams.source.topic.nodes.<TOPIC_NAME>=<PATTERN>
streams.source.topic.relationships.<TOPIC_NAME>=<PATTERN>
streams.source.topic.relationships.<TOPIC_NAME>.key_strategy=<default/all>
streams.source.enabled=<true/false, default=true>
streams.source.schema.polling.interval=<MILLIS, the polling interval for getting the schema information>
----

[NOTE]
====
**To use the Kafka transactions please set `kafka.transactional.id` and `kafka.acks` properly**.
Checkout this https://www.confluent.io/blog/transactions-apache-kafka/[blog post] for further details
about transactions in Apache Kafka
====

See the https://kafka.apache.org/documentation/#brokerconfigs[Apache Kafka documentation] for details on these settings.

In case you Kafka broker is configured with `auto.create.topics.enable` to `false`,
all the messages sent to topics that don't exist are discarded;
this because the `KafkaProducer.send()` method blocks the execution, as explained in https://issues.apache.org/jira/browse/KAFKA-3539[KAFKA-3539].
You can tune the custom property `kafka.topic.discovery.polling.interval` in order to
periodically check for new topics into the Kafka cluster so the plugin will be able
to send events to the defined topics.

With `kafka.streams.log.compaction.strategy=delete` will be generated a sequence of unique keys with Neo4j Streams Source.
instead with `kafka.streams.log.compaction.strategy=compact` the keys will be adapted to enable
https://kafka.apache.org/documentation.html#compaction[Log Compaction] on the Kafka side.
Please note that delete strategy does not actually delete records, it has this name  to match the topic config `cleanup.policy=delete/compact`.
Namely, the operations which will involve the same nodes or relationships, will have the same key.

When `kafka.streams.log.compaction.strategy=compact`, for partitioning we leverage internal Kafka mechanism.

xref:message-structure.adoc[See 'message structure' section to see key examples]

