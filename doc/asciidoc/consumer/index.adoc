
[#neo4j_streams_sink]
== Neo4j Streams - Sink: Kafka -> Neo4j
:environment: streams.sink
:id: streams_sink

ifdef::env-docs[]
[abstract]
--
This chapter describes the Neo4j Streams Sink in the Neo4j Streams Library.
Use this section to configure Neo4j to ingest the data from Kafka into Neo4j.
--
endif::env-docs[]

Is the Kafka Sink that ingest the data directly into Neo4j

=== How it works

It works in several ways:

* by providing a Cypher template
* by ingesting the events emitted from another Neo4j instance via the Change Data Capture module
* by providing a pattern extraction to a JSON or AVRO file
* by managing a CUD file format

==== Cypher Template

It works with template Cypher queries stored into properties with the following format:

.neo4j.conf
[source,ini,subs="verbatim,attributes"]
----
{environment}.topic.cypher.<TOPIC_NAME>=<CYPHER_QUERY>
----

Each Cypher template must refer to an *event* object that will be injected by the Sink

Following an example:

For this event

[source,javascript]
----
{
 "id": 42,
 "properties": {
   "title": "Answer to anyting",
   "description": "It depends."
 }
}
----

.neo4j.conf
[source,ini,subs="attributes"]
----
include::sink-cypher-template-example.adoc[]
----

Under the hood the Sink inject the event object as a parameter in this way

[source,cypher]
----
UNWIND {events} AS event
MERGE (n:Label {id: event.id})
    ON CREATE SET n += event.properties
----

Where `{batch}` is a json list, so continuing with the example above a possible full representation could be:

[source,cypher]
----
:params events => [{id:"alice@example.com",properties:{name:"Alice",age:32}},
    {id:"bob@example.com",properties:{name:"Bob",age:42}}]

UNWIND {events} AS event
MERGE (n:Label {id: event.id})
    ON CREATE SET n += event.properties
----

include::../sink-strategies/index.adoc[]

include::../cud-file-format/index.adoc[]

[#neo4j_streams_dlq]
=== How deal with bad data

The Neo4j Streams Plugin provides several means to handle processing errors.

It can fail fast or log errors with different detail levels.
Another way is to re-route all the data and errors that for something reason it wasn't able to ingest to a `Dead Letter Queue`.

NOTE: It behaves by default like Kafka Connect, see this https://www.confluent.io/blog/kafka-connect-deep-dive-error-handling-dead-letter-queues[blog post^]

* fail fast (abort) by default
* need to configure dead-letter-queue topic to enable
* need to enable logging explicitly
* headers and message logging must be enabled explicitly

Config Options

[[dlq-table]]
.Dead Letter Queue configuration parameters
[%autowidth,cols="m,m,a",opts=header]
|===
| Name | Value | Note
| errors.tolerance | none |  fail fast (default!) abort
| errors.tolerance | all |  all == lenient, silently ignore bad messages
| errors.log.enable | false/true | log errors (default: false)
| errors.log.include.messages | false/true | log bad messages too (default: false)
| errors.deadletterqueue.topic.name | topic-name | dead letter queue topic name, if left off no DLQ, default: not set
| errors.deadletterqueue.context.headers.enable | false/true | enrich messages with metadata headers like exception, timestamp, org. topic, org.part, default:false
| errors.deadletterqueue.context.headers.prefix | prefix-text | common prefix for header entries, e.g. `"__streams.errors."` , default: not set
| errors.deadletterqueue.topic.replication.factor | 3/1 | replication factor, need to set to 1 for single partition, default:3
|===

For the Neo4j extension you prefix them with `{environment}` in the Neo4j configuration.

Example settings:

.Fail Fast, Abort
[source]
----
errors.tolerance=none
----


.Don't fail on errors, Log with Messages
[source]
----
errors.tolerance=all
errors.log.enable=true
errors.log.include.messages=true
----

.Don't fail on errors, Don't log but send to DLQ with headers
[source]
----
errors.tolerance=all
errors.deadletterqueue.topic.name=my-dlq-topic
errors.deadletterqueue.context.headers.enable=true
----

.Same Settings for Neo4j Server Plugin
[source,subs="attributes"]
----
include::sink-handle-errors.adoc[]
----

Every published record in the `Dead Letter Queue` contains the original record `Key` and `Value` pairs and optionally the following headers:

[cols="1m,3a",opts=header]
|===
| Header key
| Description

| <prefix>topic
| The topic where the data is published

| <prefix>partition
| The topic partition where the data is published

| <prefix>soffset
| The offset of the data into the topic partition

| <prefix>class.name
| The class that generated the error

| <prefix>exception.class.name
| The exception that generated the error

| <prefix>exception.message
| The exception message

| <prefix>exception.stacktrace"
| The exception stack trace
|===

=== Supported Kafka deserializers

The Neo4j Streams plugin supports 2 deserializers:

* `org.apache.kafka.common.serialization.ByteArrayDeserializer`: if you want manage JSON messages
* `io.confluent.kafka.serializers.KafkaAvroDeserializer`: if you want manage AVRO messages

You can define them independently for `Key` and `Value` as specified in the Configuration paragraph

=== Configuration summary

include::configuration.adoc[]
