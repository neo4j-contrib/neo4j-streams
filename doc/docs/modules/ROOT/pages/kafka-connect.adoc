= Kafka Connect Plugin
:environment: neo4j
:id: neo4j

ifdef::env-docs[]
[abstract]
--
This chapter describes Kafka Connect plugins in the Neo4j Streams Library.
--
endif::env-docs[]

image::ROOT:neo4j-loves-confluent.png[title="Neo4j Loves Confluent", align="center"]

Kafka Connect, an open source component of Apache Kafka, is a framework for connecting Kafka with external systems such as databases, key-value stores, search indexes, and file systems.

The Neo4j Streams project provides a Kafka Connect plugin that can be installed into the Confluent Platform enabling:

- Ingest data from Kafka topics directly into Neo4j via templated Cypher queries;
- Stream Neo4j queries.

[#kafka_connect_plugin_install]
== Plugin installation

You can choose your preferred way in order to install the plugin

=== Download and install the plugin via Confluent Hub client

If you are using the provided compose file you can easily install the plugin by using the Confluent Hub.

Once the compose file is up and running you can install the plugin by executing the following command:

[source,bash]
----
<confluent_platform_dir>/bin/confluent-hub install neo4j/kafka-connect-neo4j:<version>
----

When the installation will ask:

[source,bash]
----
The component can be installed in any of the following Confluent Platform installations:
----

Please prefer the solution `(where this tool is installed)` and then go ahead with the default options.

Following an example:

image::ROOT:confluent-hub-client-installation.png[title="Installation via Confluent Hub Client", align="center"]

At the end of the process the plugin is automatically installed.


=== Download the zip from the Confluent Hub

Please go to the Confluent Hub page of the plugin:

{url-confluent-hub-neo4j}

And click to the **Download Connector** button.

Once you downloaded the file please place it into your Kafka Connect `plugins` dir.


=== Build it locally

Download the project from Github:

    git clone https://github.com/neo4j-contrib/neo4j-streams.git

Go into the `neo4j-streams` directory:

    cd neo4j-streams

Build the project by running the following command:

    mvn clean install

Inside the directory `<neo4j-streams>/kafka-connect-neo4j/target/component/packages` you'll find a file named `neo4j-kafka-connect-neo4j-<VERSION>.zip`, please unpackage and place it into your Kafka Connect `plugins` dir.

[#kafka-connect-source-instance]
== Create the Source Instance

In this chapter we'll discuss about how the Source instance works

You can create a new Source instance with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.source.avro.neo4j.json
----

Let's look at the `contrib.source.avro.neo4j.json` file:

[source,json]
----
include::ROOT:partial$docker-data/contrib.source.avro.neo4j.json[]
----

This will create a Kafka Connect Source instance that will send `AVRO` message over the topic named `my-topic`. Every message in the
topic will have the following structure:

[source,json]
----
{"name": <name>, "timestamp": <timestamp>}
----

**Nb.** Please check the <<kafka-connect-configuration-summary>> for a detailed guide about the supported configuration
parameters

=== How the Source module pushes the data to the defined Kafka topic

We use the query provided in the `neo4j.source.query` field by polling the database every value is into the
`neo4j.streaming.poll.interval.msecs` field.

So given the JSON configuration we have that we'll perform:

[source,cypher]
----
MATCH (ts:TestSource) WHERE ts.timestamp > $lastCheck RETURN ts.name AS name, ts.timestamp AS timestamp
----

every 5000 milliseconds by publishing events like:

[source,json]
----
{"name":{"string":"John Doe"},"timestamp":{"long":1624551349362}}
----

In this case we use `neo4j.enforce.schema=true` and this means that we will attach a schema for each record, in case
you want to stream pure simple JSON strings just use the relative serializer with `neo4j.enforce.schema=false` with the
following output:

[source,json]
----
{"name": "John Doe", "timestamp": 1624549598834}
----

[#kafka-connect-sink-instance]
== Create the Sink Instance

Create the Sink instance:

We'll define the Sink configuration in several ways:

* by providing a Cypher template
* by ingesting the events emitted from another Neo4j instance via the Change Data Capture module
* by providing a pattern extraction to a JSON or AVRO file
* by managing a CUD file format

=== Cypher template

[source,json]
----
include::ROOT:partial$docker-data/contrib.sink.avro.neo4j.json[]
----

In particular this line:

[source,ini,subs="verbatim,attributes"]
----
"{environment}.topic.cypher.my-topic": "MERGE (p:Person{name: event.name, surname: event.surname}) MERGE (f:Family{name: event.surname}) MERGE (p)-[:BELONGS_TO]->(f)"
----

defines that all the data that comes from the topic `my-topic` will be unpacked by the Sink into Neo4j with the following Cypher query:

[source,cypher]
----
MERGE (p:Person{name: event.name, surname: event.surname})
MERGE (f:Family{name: event.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

Under the hood the Sink inject the event object in this way

[source,cypher]
----
UNWIND $batch AS event
MERGE (p:Person{name: event.name, surname: event.surname})
MERGE (f:Family{name: event.surname})
MERGE (p)-[:BELONGS_TO]->(f)
----

Where `$batch` is a list of event objects.

You can change the query or remove the property and add your own, but you must follow the following convention:

[source,json,subs="verbatim,attributes"]
----
"{environment}.topic.cypher.<YOUR_TOPIC>": "<YOUR_CYPHER_QUERY>"
----

Let's load the configuration into the Confluent Platform with this REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.sink.avro.neo4j.json
----

The file `contrib.sink.string-json.neo4j.json` contains a configuration that manage a simple JSON producer example

Please check that everything is fine by going into:

\http://localhost:9021/management/connect

and click to the **Sink** tab. You must find a table just like this:

[cols="4*",options="header"]
|===
|Status
|Active Tasks
|Name
|Topics

|Running
|1
|Neo4jSinkConnector
|my-topic
|===

[NOTE]
====
Note that the Sink instance can be configured also to monitor multiple topics. Just evaluate the property `**topics**`
with a list of topic separated by comma. For example:
====

[source,json]
----
{
  "name": "Neo4jSinkConnector",
  "config": {
    "connector.class": "streams.kafka.connect.sink.Neo4jSinkConnector",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": false,
    "topics": "topicA,topicB",
    "_comment": "Cypher template example configuration",
    "neo4j.topic.cypher.topicA": "<YOUR_CYPHER_QUERY>",
    "neo4j.topic.cypher.topicB": "<YOUR_CYPHER_QUERY>",
    "errors.retry.timeout": "-1",
    "errors.retry.delay.max.ms": "1000",
    "errors.tolerance": "all",
    "errors.log.enable": true,
    "errors.log.include.messages": true,
    "neo4j.server.uri": "bolt://neo4j:7687",
    "neo4j.authentication.basic.username": "neo4j",
    "neo4j.authentication.basic.password": "password",
    "neo4j.encryption.enabled": false
  }
}
----


[#kafka-connect-sink-strategies]
include::sink-strategies.adoc[]

[#kafka-connect-cud-file-format]
include::cud-file-format.adoc[]

=== Multi Database Support

Neo4j 4.0 Enterprise has https://neo4j.com/docs/operations-manual/4.0/manage-databases/[multi-tenancy support],
in order to support this feature, in order to support this feature with Kafka Connect plugin, creating the Sink
instance we have to add the `neo4j.database` property, which tells the Connector the database to use as default. If you
don't specify that property, the default database `neo4j` will be used.

[NOTE]
====
Remember the naming rules for databases described https://neo4j.com/docs/operations-manual/4.0/manage-databases/configuration/#manage-databases-administration[here]
====

Following an example:

[source, json]
----
{
  "name": "Neo4jSinkConnector",
  "config": {
    "neo4j.database": "<database_name>",
    "topics": "topic",
    "connector.class": "streams.kafka.connect.sink.Neo4jSinkConnector",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": false,
    "errors.retry.timeout": "-1",
    "errors.retry.delay.max.ms": "1000",
    "errors.tolerance": "all",
    "errors.log.enable": true,
    "errors.log.include.messages": true,
    "neo4j.server.uri": "bolt://neo4j:7687",
    "neo4j.authentication.basic.username": "neo4j",
    "neo4j.authentication.basic.password": "password",
    "neo4j.encryption.enabled": false,
    "_comment": "Sink CDC SourceId Strategy",
    "neo4j.topic.cdc.sourceId": "topic",
    "neo4j.topic.cdc.sourceId.labelName": "<the label attached to the node, default=SourceEvent>",
    "neo4j.topic.cdc.sourceId.idName": "<the id name given to the CDC id field, default=sourceId>",
    "_comment": "Sink CDC Schema Strategy",
    "neo4j.topic.cdc.schema": "<list_of_topics_separated_by_semicolon>",
    "_comment": "Sink Node/Relationship Pattern Strategy",
    "neo4j.topic.pattern.node.<TOPIC_NAME>": "<node_extraction_pattern>",
    "neo4j.topic.pattern.relationship.<TOPIC_NAME>": "<relationship_extraction_pattern>",
    "_comment": "Sink CUD File forma Strategy",
    "neo4j.topic.cud": "<list_of_topics_separated_by_semicolon>"
  }
}
----

[#kafka_connect_error_handling]
== How deal with bad data

In Kafka Connect plugin, in the creation phase of the Sink instance, in addition to the properties
described in the xref:consumer.adoc#dlq-table[Dead Letter Queue configuration parameters table], you need to define kafka broker connection properties:

|===
| Name | mandatory | Description

| kafka.bootstrap.servers | true | It's the Kafka Broker url. *(please look at the description below)

| kafka.<any_other_kafka_property> | false | You can also specify any other kafka Producer
setting by adding the `kafka.` prefix (i.e the configuration `acks` become `kafka.acks`). See the https://kafka.apache.org/documentation/#brokerconfigs[Apache Kafka documentation] for details on these settings.

|===

As you may have noticed we're asking to provide the `bootstrap.server` property,
this because the Kafka Connect Framework provides an out-of-the-box support
only for deserialization errors and message transformations
(please look into the following link for further details: {url-confluent-blog}/kafka-connect-deep-dive-error-handling-dead-letter-queues/).
We want to extend this feature for transient errors in order to cover the 100% of failures.
So to do that at this moment as suggested by Confluent we need to ask again the broker location,
until this JIRA issue will not be addressed: https://issues.apache.org/jira/browse/KAFKA-8597.
Said that, these properties has to be added only if you want to also redirect Neo4j errors into the DLQ.

[#kafka_connect_monitor]
== Monitor via Confluent Platform UI

The Kafka Monitoring UI can be found at \http://<localhost>:9021/management/connect

image::ROOT:confluent-metrics.png[title="Confluent Importing Metrics", align="center"]

They show up properly in my topic, and then are added to Neo4j via the sink.

Below you see the data that has been ingested into Neo4j. During my testing I got up to more than 2M events.

image::ROOT:confluent-imported-data.png[title="Confluent Platform Management", align="center"]

include::config-override-policy.adoc[]

[#kafka-connect-configuration-summary]
== Configuration Summary

Following a summary of all the configuration parameters you can use for the Kafka Connect plugin:

.Kafka Connect Common configuration parameters (shared between Source and Sink)
[%width="100%",cols="m,m,m,a", opts=header]
|===
| Name
| Value
| Mandatory
| Note

| connector.class | streams.kafka.connect.sink.Neo4jSinkConnector | true |
| key.converter | org.apache.kafka.connect.storage.StringConverter | false | Converter class for key Connect data
| value.converter | org.apache.kafka.connect.json.JsonConverter | false | Converter class for value Connect data
| key.converter.schemas.enable | true/false | false | If true the key will be treated as a composite JSON object containing schema and the data. Default value is false
| value.converter.schemas.enable | true/false | false | If true the value will be treated as a composite JSON object containing schema and the data. Default value is false
| key.converter.schema.registry.url | \http://localhost:8081 | false | The Schema Registry URL has to be provide only when you decide to use AvroConverter
| value.converter.schema.registry.url | \http://localhost:8081 | false | The Schema Registry URL has to be provide only when you decide to use AvroConverter
| neo4j.database | "bolt://neo4j:7687" | true | Specify a database name only if you want to use a non-default database. Default value is 'neo4j'
| neo4j.server.uri | "bolt://neo4j:7687" | true | Neo4j Server URI
| neo4j.authentication.basic.username | your_neo4j_user | true | Neo4j username
| neo4j.authentication.basic.password | your_neo4j_password | true | Neo4j password
| neo4j.authentication.basic.realm | your_neo4j_auth_realm | false | The authentication realm
| neo4j.authentication.kerberos.ticket | your_kerberos_ticket | false | The Kerberos ticket
| neo4j.authentication.type | NONE/BASIC/KERBEROS | false | The authentication type (default: 'BASIC')
| neo4j.batch.size | Integer | false | The max number of events processed by the Cypher query (default: 1000)
| neo4j.batch.timeout.msecs | Integer | false | The execution timeout for the cypher query (default: 0, that is without timeout)
| neo4j.connection.max.lifetime.msecs | Long | false | The max Neo4j connection lifetime (default: 1 hour)
| neo4j.connection.acquisition.timeout.msecs | Long | false | The max Neo4j acquisition timeout (default 1 hour)
| neo4j.connection.liveness.check.timeout.msecs | Long | false | The max Neo4j liveness check timeout (default 1 hour)
| neo4j.connection.max.pool.size | Int | false | The max pool size (default: 100)
| neo4j.encryption.ca.certificate.path | your_certificate_path | false | The path of the certificate
| neo4j.encryption.enabled | true/false | false |
| neo4j.encryption.trust.strategy | TRUST_ALL_CERTIFICATES/TRUST_CUSTOM_CA_SIGNED_CERTIFICATES/TRUST_SYSTEM_CA_SIGNED_CERTIFICATES | false | The Neo4j trust strategy (default: TRUST_ALL_CERTIFICATES)
| neo4j.retry.backoff.msecs | Long | false | The time in milliseconds to wait following a transient error before a retry attempt is made (default: 30000).
| neo4j.retry.max.attemps | Long | false | The maximum number of times to retry on transient errors (except for TimeoutException) before failing the task (default: 5).
|===

.Kafka Connect Sink configuration parameters
[%width="100%",cols="m,m,m,a", opts=header]
|===
| Name
| Value
| Mandatory
| Note

| topics | <topicA,topicB> | true | A list of comma-separated topics
| kafka.bootstrap.servers | <localhost:9092> | false | The Broker URI is mandatory only when if you have configured DLQ
| kafka.<any_other_kafka_property> | | false |
| errors.tolerance | all/none | false | all == lenient, silently ignore bad messages. none (default) means that any error will result in a connector failure
| errors.log.enable | false/true | false | log errors (default: false)
| errors.log.include.messages | false/true | false | log bad messages too (default: false)
| errors.deadletterqueue.topic.name | topic-name | false | dead letter queue topic name, if left off no DLQ, default: not set
| errors.deadletterqueue.context.headers.enable | false/true | false | enrich messages with metadata headers like exception, timestamp, org. topic, org.part, default:false
| errors.deadletterqueue.context.headers.prefix | prefix-text | false | common prefix for header entries, e.g. `"__streams.errors."` , default: not set
| errors.deadletterqueue.topic.replication.factor | 3/1 | false | replication factor, need to set to 1 for single partition, default:3
| neo4j.batch.parallelize | boolean | false | If enabled messages are processed concurrently in the sink. Non concurrent execution supports in-order processing, e.g. for CDC
| neo4j.topic.cdc.sourceId | <list of topics separated by semicolon> | false |
| neo4j.topic.cdc.sourceId.labelName | <the label attached to the node> | false | default value is *SourceEvent*
| neo4j.topic.cdc.sourceId.idName | <the id name given to the CDC id field> | false | default value is *sourceId*
| neo4j.topic.cdc.schema | <list of topics separated by semicolon> | false |
| neo4j.topic.pattern.node.<TOPIC_NAME> | <node extraction pattern> | false |
| neo4j.topic.pattern.relationship.<TOPIC_NAME> | <relationship extraction pattern> | false |
| neo4j.topic.cud | <list of topics separated by semicolon> | false |
| neo4j.topic.cypher.<topic> | <valid Cypher query> | false |
|===

.Kafka Connect Source configuration parameters
[%width="100%",cols="m,m,m,a", opts=header]
|===
| Name
| Value
| Mandatory
| Note

| topic | <topic> | true | The topic where the Source will publish the data
| partitions | <parition> | false | The number of partition for the Source (default 1)
| neo4j.streaming.from | enum[ALL, NOW, LAST_COMMITTED] | false | When start the Source. ALL means from the beginning.
LAST_COMMITTED will try to retrieve already committed offset, in case it will not find one LAST_COMMITTED use NOW as fallback (default NOW)
| neo4j.source.type | enum[QUERY] | false | The type of the Source strategy, with QUERY you must set `neo4j.source.query`
| neo4j.source.query | <valid cypher query> | false | The Cypher query in order to extract the data from Neo4j you need to define it if you use `neo4j.source.type=QUERY`
| neo4j.streaming.property | <property> | false | The name of the property that we need to consider in order to determinate the last queried record;
if not defined we use an internal value given from the last performed check.
We use this value for injecting it in the provided query defined in `neo4j.source.query` as `$lastChek` parameter
| neo4j.streaming.poll.interval.msecs | <millis> | Int | The polling interval in ms (Default: 10000)
| neo4j.enforce.schema | <true/false> | false | Apply a schema to each record (Default: false)
|===

[NOTE]
====
If you need to manage data in JSON format without using the Schema Registry, then you can use the
`org.apache.kafka.connect.json.JsonConverter` and disabling both `key.converter.schemas.enable` and
`value.converter.schemas.enable`.

Other supported converters are:

* *org.apache.kafka.connect.storage.StringConverter*
* *org.apache.kafka.connect.converters.ByteArrayConverter*
* *io.confluent.connect.avro.AvroConverter*

Please see the following for further details: https://docs.confluent.io/home/connect/userguide.html#connect-configuring-converters

For further information about Kafka Connect properties, please checkout the following:

* {url-confluent-install}/configuration/connect/sink-connect-configs.html
* {url-confluent-install}/configuration/connect/source-connect-configs.html

For further details about error handling properties refers to <<kafka_connect_error_handling, How deal with bad data>> section
====

Kafka Connect plugin supports also the secured Neo4j URI schemes.
Please see the Neo4j official documentation for detailed information: https://neo4j.com/docs/driver-manual/current/client-applications/#driver-configuration-examples
