= Neo4j Streams - Sink: Kafka -> Neo4j
:environment: streams.sink
:id: streams_sink

ifdef::env-docs[]
[abstract]
--
This chapter describes the Neo4j Streams Sink in the Neo4j Streams Library.
Use this section to configure Neo4j to ingest the data from Kafka into Neo4j.
--

endif::env-docs[]

include::deprecation.adoc[]

[[neo4j_streams_sink]]
Is the Kafka Sink that ingest the data directly into Neo4j

[[neo4j_streams_sink_howitworks]]
== How it works

It works in several ways:

* by providing a Cypher template
* by ingesting the events emitted from another Neo4j instance via the Change Data Capture module
* by providing a pattern extraction to a JSON or AVRO file
* by managing a CUD file format

=== Cypher Template

It works with template Cypher queries stored into properties with the following format:

.neo4j.conf
[source,ini,subs="verbatim,attributes"]
----
{environment}.topic.cypher.<TOPIC_NAME>=<CYPHER_QUERY>
----

Each Cypher template must refer to an *event* object that will be injected by the Sink

Following an example:

For this event

[source,javascript]
----
{
 "id": 42,
 "properties": {
   "title": "Answer to anyting",
   "description": "It depends."
 }
}
----

.neo4j.conf
[source,ini,subs="attributes"]
----
{environment}.topic.cypher.my-topic=MERGE (n:Label {id: event.id}) \
ON CREATE SET n += event.properties
----

Under the hood the Sink inject the event object as a parameter in this way

[source,cypher]
----
UNWIND {events} AS event
MERGE (n:Label {id: event.id})
    ON CREATE SET n += event.properties
----

Where `\{events}` is a json list, so continuing with the example above a possible full representation could be:

[source,cypher]
----
:params events => [{id:"alice@example.com",properties:{name:"Alice",age:32}},
    {id:"bob@example.com",properties:{name:"Bob",age:42}}]

UNWIND {events} AS event
MERGE (n:Label {id: event.id})
    ON CREATE SET n += event.properties
----

[NOTE]
====
When you decide to use Cypher template as Sink strategy to import data from Kafka into Neo4j, you have to be sure
about the query correctness. If the query is not optimized, this could also results into possible performance issue or
in situations where the plugin seems to be stuck, for example if the query loads a large amount of nodes and relationships
into memory.
We suggests the following:

* execute a query **EXPLAIN** in order to better analyze the query and avoid this kind of situations

* if Neo4j seems to be stuck then, from the Neo4j Browser, execute a `CALL dbms.listQueries()` to view all queries that
are currently executing within the instance, and to be sure that there are no locked queries
====

include::sink-strategies.adoc[]

include::cud-file-format.adoc[]

[[neo4j_streams_dlq]]
== How deal with bad data

The Neo4j Streams Plugin provides several means to handle processing errors.

It can fail fast or log errors with different detail levels.
Another way is to re-route all the data and errors that for something reason it wasn't able to ingest to a `Dead Letter Queue`.

NOTE: It behaves by default like Kafka Connect, see this {url-confluent-blog}/kafka-connect-deep-dive-error-handling-dead-letter-queues/[blog post^]

* fail fast (abort) by default
* need to configure dead-letter-queue topic to enable
* need to enable logging explicitly
* headers and message logging must be enabled explicitly

Config Options

[[dlq-table]]
.Dead Letter Queue configuration parameters
[%autowidth,cols="m,m,a",opts=header]
|===
| Name | Value | Note
| errors.tolerance | none |  fail fast (default!) abort
| errors.tolerance | all |  all == lenient, silently ignore bad messages
| errors.log.enable | false/true | log errors (default: false)
| errors.log.include.messages | false/true | log bad messages too (default: false)
| errors.deadletterqueue.topic.name | topic-name | dead letter queue topic name, if left off no DLQ, default: not set
| errors.deadletterqueue.context.headers.enable | false/true | enrich messages with metadata headers like exception, timestamp, org. topic, org.part, default:false
| errors.deadletterqueue.context.headers.prefix | prefix-text | common prefix for header entries, e.g. `"__streams.errors."` , default: not set
| errors.deadletterqueue.topic.replication.factor | 3/1 | replication factor, need to set to 1 for single partition, default:3
|===

For the Neo4j extension you prefix them with `{environment}` in the Neo4j configuration.

Example settings:

.Fail Fast, Abort
[source]
----
errors.tolerance=none
----


.Don't fail on errors, Log with Messages
[source]
----
errors.tolerance=all
errors.log.enable=true
errors.log.include.messages=true
----

.Don't fail on errors, Don't log but send to DLQ with headers
[source]
----
errors.tolerance=all
errors.deadletterqueue.topic.name=my-dlq-topic
errors.deadletterqueue.context.headers.enable=true
----

.Same Settings for Neo4j Server Plugin
[source,subs="attributes"]
----
{environment}.errors.tolerance=all
{environment}.errors.deadletterqueue.topic.name=my-dlq-topic
{environment}.errors.deadletterqueue.context.headers.enable=true
----

Every published record in the `Dead Letter Queue` contains the original record `Key` and `Value` pairs and optionally the following headers:

[cols="1m,3a",opts=header]
|===
| Header key
| Description

| <prefix>topic
| The topic where the data is published

| <prefix>partition
| The topic partition where the data is published

| <prefix>soffset
| The offset of the data into the topic partition

| <prefix>class.name
| The class that generated the error

| <prefix>exception.class.name
| The exception that generated the error

| <prefix>exception.message
| The exception message

| <prefix>exception.stacktrace"
| The exception stack trace

| <prefix>databaseName"
| The database name
|===

[[neo4j_streams_supported_deserializers]]
== Supported Kafka deserializers

The Neo4j Streams plugin supports 2 deserializers:

* `org.apache.kafka.common.serialization.ByteArrayDeserializer`: if you want manage JSON messages
* `io.confluent.kafka.serializers.KafkaAvroDeserializer`: if you want manage AVRO messages

You can define them independently for `Key` and `Value` as specified in the Configuration paragraph

include::consumer-configuration.adoc[]
