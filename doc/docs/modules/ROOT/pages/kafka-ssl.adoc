
[[kafka_ssl]]
== Configure with Kafka over SSL

This section provides guidance to configure SSL security between Kafka and Neo4j. This will provide data encryption
between Kafka and Neo4j.

This does not address ACL confguration inside of KAFKA.

Please see also the following Confluent documentations for further details on how to configure encryption and authentication with SSL:

* https://docs.confluent.io/current/kafka/authentication_sasl/index.html[Authentication with SASL]

* https://docs.confluent.io/current/kafka/authentication_ssl.html[Encryption and Authentication with SSL]

[[kafka_ssl_self_signed]]
=== Self Signed Certificates

This section came from https://medium.com/talking-tech-all-around/how-to-enable-and-verify-client-authentication-in-kafka-21e936e670e8.

Make sure that you have truststore and keystore JKSs for each server.
In case you want a self signed certificate, you can use the following commands:

[source, bash]
----
mkdir security
cd security

export PASSWORD=password
keytool -keystore kafka.server.keystore.jks -alias localhost -validity 365 -genkey
openssl req -new -x509 -keyout ca-key -out ca-cert -days 365
keytool -keystore kafka.server.truststore.jks -alias CARoot -import -file ca-cert
keytool -keystore kafka.client1.truststore.jks -alias CARoot -import -file ca-cert
keytool -keystore kafka.server.keystore.jks -alias localhost -certreq -file cert-file
openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 365 -CAcreateserial -passin pass:$PASSWORD
keytool -keystore kafka.server.keystore.jks -alias CARoot -import -file ca-cert
keytool -keystore kafka.server.keystore.jks -alias localhost -import -file cert-signed
keytool -keystore kafka.client1.keystore.jks -alias localhost -validity 365 -genkey
keytool -keystore kafka.client1.keystore.jks -alias localhost -certreq -file cert-file
openssl x509 -req -CA ca-cert -CAkey ca-key -in cert-file -out cert-signed -days 365 -CAcreateserial -passin pass:$PASSWORD
keytool -keystore kafka.client1.keystore.jks -alias CARoot -import -file ca-cert
keytool -keystore kafka.client1.keystore.jks -alias localhost -import -file cert-signed
----

Once the keystores are created, you have to move the `kafka.client1.keystore.jks` and `kafka.client1.truststore.jks` to your neo4j server.

[NOTE]
This article discusses addressing this error (Caused by: java.security.cert.CertificateException:
No subject alternative names present) that may appear when querying the topic. https://geekflare.com/san-ssl-certificate/

[[kafka_ssl_config]]
=== Kafka Configuration

Connect to your Kafka server and modify the `config/server.properties` file.
This configuration worked in general but other configurations without the EXTERNAL and INTERNAL settings should works as well.
This configuration is for Kafka on AWS but should work for other configurations.

[source, properties]
----
listeners=EXTERNAL://0.0.0.0:9092,INTERNAL://0.0.0.0:19092,CLIENT://0.0.0.0:9093,SSL://0.0.0.0:9094
listener.security.protocol.map=EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT,CLIENT:PLAINTEXT,SSL:SSL
advertised.listeners=EXTERNAL://aws_public_ip:9092,INTERNAL://aws_internal_ip:19092,CLIENT://aws_public_ip:9093,SSL://aws_public_ip:9094
inter.broker.listener.name=INTERNAL

ssl.keystore.location=/home/kafka/security/kafka.server.keystore.jks
ssl.keystore.password=neo4jpassword
ssl.truststore.location=/home/kafka/security/kafka.server.truststore.jks
ssl.truststore.password=neo4jpassword
ssl.key.password=neo4jpassword
ssl.enabled.protocols=TLSv1.2,TLSv1.1

ssl.endpoint.identification.algorithm=HTTPS
ssl.client.auth=required
----

[[kafka_ssl_neo4j_config]]
=== Neo4j Configuration

The following is required for a Neo4j configuration. In this case, we are connecting to the public AWS IP address.
The keystore and truststore locations point to the files that you created earlier in the steps.

Note that the passwords are stored in plaintext so limit access to this `neo4j.conf` file.

[source, properties]
----
kafka.zookeeper.connect=xxx.xxx.xxx.xxx:2181
kafka.bootstrap.servers=xxx.xxx.xxx.xxx:9094
streams.sink.enabled=false

streams.source.topic.nodes.neoTest=Person{*}

kafka.auto.offset.reset=earliest
kafka.group.id=neo4j
streams.sink.dlq=neo4j-dlq

kafka.acks=all
kafka.num.partitions=1
kafka.retries=2
kafka.batch.size=16384
kafka.buffer.memory=33554432

kafka.security.protocol=SSL
kafka.ssl.truststore.location=/home/ubuntu/security/kafka.client1.truststore.jks
kafka.ssl.truststore.password=neo4jpassword
kafka.ssl.keystore.location=/home/ubuntu/security/kafka.client1.keystore.jks
kafka.ssl.keystore.password=neo4jpassword
kafka.ssl.key.password=neo4jpassword
kafka.ssl.endpoint.identification.algorithm=HTTPS

dbms.security.procedures.whitelist=apoc.*
dbms.security.procedures.unrestricted=apoc.*
dbms.jvm.additional=-Djavax.net.debug=ssl:handshake
----

This line `*dbms.jvm.additional=-Djavax.net.debug=ssl:handshake*` is optional but does help for debugging SSL issues.

[NOTE]
====
When configuring a secure connection between Neo4j and Kafka, and using SASL protocol in particular, pay attention to
use the following properties:

[source, properties]
----
kafka.security.protocol=SASL_SSL
sasl.mechanism=GSSAPI
----

and *not* the following, which has to be used on server side and not client side:

[source, properties]
----
security.inter.broker.protocol=SASL_SSL
sasl.mechanism.inter.broker.protocol=GSSAPI
----

====

[[kafka_ssl_testing]]
=== Testing

After starting Kafka and Neo4j, you can test by creating a Person node in Neo4j and then query the topic as follows:

[source,bash]
----
./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic neoTest --from-beginning
----

If you want to test using SSL, you would do the following:

* Create a `client-ssl.properties` file consisting of:

[source, properties]
----
security.protocol=SSL
ssl.truststore.location=/home/kafka/security/kafka.client.truststore.jks
ssl.truststore.password=neo4jpassword
ssl.endpoint.identification.algorithm=
----

=== Authentication with SASL

You can configure JAAS by providing a JAAS configuration file. To do this, connect to your Kafka server and modify the
`config/server.properties` file. This configuration worked in general, but other configurations without the EXTERNAL
and INTERNAL settings should works as well.

This configuration, for example, is for Kafka on AWS but should work for other configurations.

[source, properties]
----
listeners=EXTERNAL://0.0.0.0:9092,INTERNAL://0.0.0.0:9093,CLIENT://0.0.0.0:9094
listener.security.protocol.map=EXTERNAL:SASL_PLAINTEXT,INTERNAL:PLAINTEXT,CLIENT:SASL_PLAINTEXT

advertised.listeners=EXTERNAL://18.188.84.xxx:9092,INTERNAL://172.31.43.xxx:9093,CLIENT://18.188.84.xxx:9094

zookeeper.connect=18.188.84.xxx:2181

sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN
inter.broker.listener.name=INTERNAL
----

On the Neo4j side the following is required. Please consider that in this case, we are connecting to the public
AWS IP address.

. Copy the contents of `~/kafka/conf/kafka_jaas.conf` on your Kafka server and save it to a file on your Neo4j server
(i.e ~/conf/kafka_client_jaas.conf)

. In *neo4j.conf*, add the following:

+
[source, properties]
----
dbms.jvm.additional=-Djava.security.auth.login.config=/Users/davidfauth/neo4j-enterprise-4.0.4_kafka/conf/kafka_client_jaas.conf
kafka.security.protocol=SASL_PLAINTEXT
kafka.sasl.mechanism=PLAIN
----

For more information, please consult the official Confluent documentation at the following links:

* https://docs.confluent.io/2.0.0/kafka/sasl.html#authentication-using-sasl
* https://docs.confluent.io/2.0.0/kafka/sasl.html#configuring-kafka-clients