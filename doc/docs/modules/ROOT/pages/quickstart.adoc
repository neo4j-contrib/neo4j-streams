= Quick Start

[[quickstart]]

ifdef::env-docs[]
[abstract]
--
Get started fast for common scenarios, using Neo4j Streams plugin or Kafka Connect Neo4j Connector
--
endif::env-docs[]

[[kafka_connect_neo4j_connector_quickstart]]
== Kafka Connect Neo4j Connector

=== Install the Connector

Download and install the plugin via Confluent Hub client. See the chapter xref:kafka-connect.adoc[Kafka Connect Neo4j Connector] for more details.

=== Run with Docker

Inside the directory `/neo4j-kafka-connect-neo4j-<version>/doc/docker` you'll find a compose file that allows you to start the whole testing environment.

.docker-compose.yml
[source,yaml]
----
include::ROOT:partial$docker-data/quickstart-kafka-connect-docker-compose.yml[]
----

Just go inside that folder from the terminal and run the following command:

[source,bash]
----
docker-compose up -d
----

When the process is terminated you have all the modules up and running:

* Neo4j
* Zookeeper
* Kafka Broker
* Schema Registry
* Kafka Connect
* Kafka Control Center

Now you can access your Neo4j instance under: \http://localhost:7474, log in with `neo4j` as username and
`connect` as password (see the docker-compose file to change it).

=== Configure SINK instance

On the Kafka Connect side only one thing is missing, namely create the SINK instance. So let's do the following REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.sink.avro.neo4j.json
----

In this case, we are configuring the SINK instance to consume and deliver data in AVRO format.
Now you can access your Confluent Control Center instance under: \http://localhost:9021/clusters,
and check the created `my-topic` as specified into the `contrib.sink.avro.neo4j.json`.

.contrib.sink.avro.neo4j.json
[source,json]
----
include::ROOT:partial$docker-data/quickstart-contrib.sink.avro.neo4j.json[]
----

The property `neo4j.topic.cypher.my-topic` defines each message that will be consumed by the SINK on
the Kafka Connect side, will cause the execution of the specified cypher query on the Neo4j side.

=== Configure SOURCE instance

On the Kafka Connect side only one thing is missing, namely create the SOURCE instance. So let's do the following REST call:

[source,shell]
----
curl -X POST http://localhost:8083/connectors \
  -H 'Content-Type:application/json' \
  -H 'Accept:application/json' \
  -d @contrib.source.avro.neo4j.json
----

This will create a Kafka Connect Source instance that will send `AVRO` message over the topic named `my-topic`.


.contrib.source.avro.neo4j.json
[source,json]
----
include::ROOT:partial$docker-data/quickstart-contrib.source.avro.neo4j.json[]
----

The property `topic` defines where each message will be pushed, the message structure
is related is given by `RETURN` cause of the Cypher statement defined into the `neo4j.source.query` property.
So given the JSON above the structure of the message will be the following:

[source,json]
----
{"name": <name>, "timestamp": <timestamp>}
----

[[neo4j_streams_plugin_quickstart]]
== Neo4j Streams Plugin

[NOTE]
The Neo4j Streams Plugin running inside the Neo4j database is deprecated and will not be supported after version 4.3 of Neo4j.
We recommend users not to adopt this plugin for new implementations, and to consider migrating to the use of the Kafka Connect Neo4j Connector as a replacement

Any configuration option that starts with `streams.` controls how the plugin itself behaves. For a full
list of options available, see the documentation subsections on the xref:producer.adoc[source] and xref:consumer.adoc#neo4j_streams_sink[sink].

=== Install the Plugin

* Download the latest release jar from https://github.com/neo4j-contrib/neo4j-streams/releases/latest
* Copy it into `$NEO4J_HOME/plugins` and configure the relevant connections

[[kafka-settings]]
=== Kafka settings

Any configuration option that starts with `kafka.` will be passed to the underlying Kafka driver. Neo4j
streams uses the official Confluent Kafka producer and consumer java clients.
Configuration settings which are valid for those connectors will also work for Neo4j Streams.

For example, in the Kafka documentation linked below, the configuration setting named `batch.size` should be stated as
`kafka.batch.size` in Neo4j Streams.

The following are common configuration settings you may wish to use.  _This is not a complete
list_.  The full list of configuration options and reference material is available from Confluent's
site for link:{url-confluent-install}/configuration/consumer-configs.html[sink configurations] and
link:{url-confluent-install}/configuration/producer-configs.html[source configurations].

.Most Common Needed Configuration Settings
|===
|Setting Name |Description |Default Value

|kafka.max.poll.records
|The maximum number of records to pull per batch from Kafka. Increasing this number will mean
larger transactions in Neo4j memory and may improve throughput.
|500

|kafka.buffer.memory
|The total bytes of memory the producer can use to buffer records waiting.  Use this to adjust
how much memory the plugin may require to hold messages not yet delivered to Neo4j
|33554432

|kafka.batch.size
|(Producer only) The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. This helps performance on both the client and the server. This configuration controls the default batch size in bytes.
|16384

|kafka.max.partition.fetch.bytes
|(Consumer only) The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer. If the first record batch in the first non-empty partition of the fetch is larger than this limit, the batch will still be returned to ensure that the consumer can make progress.
|1048576

|kafka.group.id
|A unique string that identifies the consumer group this consumer belongs to.
|N/A
|===

=== Configure Kafka Connection

If you are running locally or against a standalone machine, configure `neo4j.conf` to point to that server:

.neo4j.conf
[source,ini]
----
kafka.bootstrap.servers=localhost:9092
----

If you are using Confluent Cloud (managed Kafka), you can connect to Kafka as described in
the xref:cloud.adoc#confluent_cloud[Confluent Cloud] section

=== Decide: Sink, Source, or Both

Configuring neo4j-streams comes in three different parts, depending on your need:

. *Required*: Configuring a connection to Kafka

.neo4j.conf
[source,ini]
----
kafka.bootstrap.servers=localhost:9092
----

. _Optional_: Configuring Neo4j to produce records to Kafka (xref:producer.adoc[Source])
. _Optional_: Configuring Neo4j to ingest from Kafka (xref:consumer.adoc#neo4j_streams_sink[Sink])

Follow one or both subsections according to your use case and need:

==== Sink

Take data from Kafka and store it in Neo4j (Neo4j as a data consumer) by adding configuration such as:

.neo4j.conf
[source,ini]
----
streams.sink.enabled=true
streams.sink.topic.cypher.my-ingest-topic=MERGE (n:Label {id: event.id}) ON CREATE SET n += event.properties
----

This will process every message that comes in on `my-ingest-topic` with the given cypher statement.  When
that cypher statement executes, the `event` variable that is referenced will be set to the message received,
so this sample cypher will create a `(:Label)` node in the graph with the given ID, copying all of the
properties in the source message.

For full details on what you can do here, see the xref:consumer.adoc#neo4j_streams_sink[Sink] section of the documentation.

==== Source

Produce data from Neo4j and send it to a Kafka topic (Neo4j as a data producer) by adding configuration such as:

.neo4j.conf
[source,ini]
----
streams.source.topic.nodes.my-nodes-topic=Person{*}
streams.source.topic.relationships.my-rels-topic=BELONGS-TO{*}
streams.source.enabled=true
streams.source.schema.polling.interval=10000
----

This will produce all graph nodes labeled `(:Person)` on to the topic `my-nodes-topic` and all
relationships of type `-[:BELONGS-TO]->` to the topic named `my-rels-topic`.  Further, schema changes will
be polled every 10,000 ms, which affects how quickly the database picks up new indexes/schema changes.
Please note that if not specified a value for `streams.source.schema.polling.interval` property then Streams plugin will use
300,000 ms as default.

The expressions `Person{\*}` and `BELONGS-TO{*}` are _patterns_.  You can find documentation on how to change
these in the xref:producer.adoc#source-patterns[Patterns] section.

For full details on what you can do here, see the xref:producer.adoc[Source] section of the documentation.

==== Restart Neo4j

Once the plugin is installed and configured, restarting the database will make it active.
If you have configured Neo4j to consume from kafka, it will begin immediately processing messages.

[NOTE]

====
When installing the latest version of the Neo4j Streams plugin into Neo4j 4.x, watching to logs you could find something
similar to the following:

[source,logs]
----
2020-03-25 20:13:50.606+0000 WARN  Unrecognized setting. No declared setting with name: kafka.max.partition.fetch.bytes
2020-03-25 20:13:50.608+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.errors.log.include.messages
2020-03-25 20:13:50.608+0000 WARN  Unrecognized setting. No declared setting with name: kafka.auto.offset.reset
2020-03-25 20:13:50.608+0000 WARN  Unrecognized setting. No declared setting with name: kafka.bootstrap.servers
2020-03-25 20:13:50.608+0000 WARN  Unrecognized setting. No declared setting with name: kafka.max.poll.records
2020-03-25 20:13:50.609+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.errors.log.enable
2020-03-25 20:13:50.609+0000 WARN  Unrecognized setting. No declared setting with name: streams.source.enabled
2020-03-25 20:13:50.609+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.topic.cypher.boa.to.kafkaTest
2020-03-25 20:13:50.609+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.errors.tolerance
2020-03-25 20:13:50.609+0000 WARN  Unrecognized setting. No declared setting with name: kafka.group.id
2020-03-25 20:13:50.609+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.errors.deadletterqueue.context.headers.enable
2020-03-25 20:13:50.609+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.errors.deadletterqueue.context.header.prefix
2020-03-25 20:13:50.610+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.errors.deadletterqueue.topic.name
2020-03-25 20:13:50.610+0000 WARN  Unrecognized setting. No declared setting with name: streams.sink.enabled.to.kafkaTest
----

*These are not errors*. They comes from the new Neo4j 4 Configuration System, which warns that it doesn't recognize those
properties. Despite these warnings the plugin will work properly.
====
