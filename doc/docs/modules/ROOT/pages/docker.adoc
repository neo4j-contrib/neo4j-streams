= Run with Docker

[NOTE]
The Neo4j Streams Plugin running inside the Neo4j database is deprecated and will not be supported after version 4.2 of Neo4j.
We recommend users not to adopt this plugin for new implementations, and to consider migrating to the use of the Kafka Connect Neo4j Connector as a replacement

ifdef::env-docs[]
[abstract]
--
This chapter describes Docker Compose templates that can be used to test Neo4j Streams applications.
--
endif::env-docs[]

[[neo4j_streams_docker]]
== Neo4j Streams plugin

=== Introduction

When Neo4j is run in a Docker, some special considerations apply; please see
link:https://neo4j.com/docs/operations-manual/current/docker/configuration/[Neo4j Docker Configuration]
for more information.  In particular, the configuration format used in `neo4j.conf` looks different.

Please note that the Neo4j Docker image use a naming convention; you can override every neo4j.conf property by prefix it with `NEO4J_` and using the following transformations:

* single underscore is converted in double underscore: `_ -> __`
* point is converted in single underscore: `.` -> `_`

Example:

* `dbms.memory.heap.max_size=8G` -> `NEO4J_dbms_memory_heap_max__size: 8G`
* `dbms.logs.debug.level=DEBUG` -> `NEO4J_dbms_logs_debug_level: DEBUG`

For more information and examples see this section and the xref:examples.adoc#confluent_docker_example[Confluent With Docker] section of the documentation.

[NOTE]
====
Another important thing to watch out for is about possible permissions issue.
If you want to running Kafka in Docker using a host volume for which the user is not the owner
then you will have a permission error. There are two possible solutions:

- use a root-user
- change permissions of the volume in order to make it accessible by the non-root user
====

[NOTE]
The Neo4j docker container is built on an approach that uses environment variables passed to
the container as a way to configure Neo4j.  There are certain characters which environment variables cannot contain,
notably the dash `-` character.  Configuring the plugin to use stream names that contain these characters will not
work properly, because a configuration environment variable such as `NEO4J_streams_sink_topic_cypher_my-topic` cannot
be correctly evaluated as an environment variable (`my-topic`).  This is a limitation of the Neo4j docker container
rather than neo4j-streams.

Please note that the Neo4j Docker image use a naming convention; you can override every `neo4j.conf` property by prefix it with `NEO4J_` and using the following transformations:

* single underscore is converted in double underscore: `_ -> __`
* point is converted in single underscore: `.` -> `_`

Example:

* `dbms.memory.heap.max_size=8G` -> `NEO4J_dbms_memory_heap_max__size: 8G`
* `dbms.logs.debug.level=DEBUG` -> `NEO4J_dbms_logs_debug_level: DEBUG`

Following you'll find a lightweight Docker Compose file that allows you to test the application in your local environment

Prerequisites:

- Docker
- Docker Compose

Here the instruction about how to configure https://docs.docker.com/compose/install/[Docker and Docker-Compose]

From the same directory where the compose file is, you can launch this command:

[source,bash]
----
docker-compose up -d
----

=== Source module

Following a compose file that allows you to spin-up Neo4j, Kafka and Zookeeper in order to test the application.

.docker-compose.yml
[source,yaml]
----
include::ROOT:partial$docker-data/docker-compose.yml[]
----

==== Launch it locally

===== Prerequisites

* Install the latest version of Neo4j Streams plugin into `./neo4j/plugins`

Before starting please change the volume directory according to yours, inside the <plugins> dir you must put Streams jar

[source,yml]
----
volumes:
    - ./neo4j/plugins:/plugins
----

You can execute a Kafka Consumer that subscribes the topic `neo4j` by executing this command:

[source,bash]
----
docker exec kafka kafka-console-consumer --bootstrap-server kafka:19092 --topic neo4j --from-beginning
----

Then directly from the Neo4j browser you can generate some random data with this query:

[source,cypher]
----
UNWIND range(1,100) as id
CREATE (p:Person {id:id, name: "Name " + id, age: id % 3}) WITH collect(p) as people
UNWIND people as p1
UNWIND range(1,10) as friend
WITH p1, people[(p1.id + friend) % size(people)] as p2
CREATE (p1)-[:KNOWS {years: abs(p2.id - p1.id)}]->(p2)
----

And if you go back to your consumer you'll see something like this:

[source,json]
----

{"meta":{"timestamp":1571329239766,"username":"neo4j","txId":20,"txEventId":98,"txEventsCount":1100,"operation":"created","source":{"hostname":"neo4j"}},"payload":{"id":"84","before":null,"after":{"properties":{"name":"Name 85","id":85,"age":1},"labels":["Person"]},"type":"node"},"schema":{"properties":{"name":"String","id":"Long","age":"Long"},"constraints":[]}}

{"meta":{"timestamp":1571329239766,"username":"neo4j","txId":20,"txEventId":99,"txEventsCount":1100,"operation":"created","source":{"hostname":"neo4j"}},"payload":{"id":"85","before":null,"after":{"properties":{"name":"Name 86","id":86,"age":2},"labels":["Person"]},"type":"node"},"schema":{"properties":{"name":"String","id":"Long","age":"Long"},"constraints":[]}}

{"meta":{"timestamp":1571329239766,"username":"neo4j","txId":20,"txEventId":100,"txEventsCount":1100,"operation":"created","source":{"hostname":"neo4j"}},"payload":{"id":"0","start":{"id":"0","labels":["Person"],"ids":{}},"end":{"id":"2","labels":["Person"],"ids":{}},"before":null,"after":{"properties":{"years":2}},"label":"KNOWS","type":"relationship"},"schema":{"properties":{"years":"Long"},"constraints":[]}}

----

[NOTE]
Please note that in this example no topic name was specified before the execution of the Kafka Consumer, which is listening on `neo4j` topic.
This is because Neo4j Streams plugin, if not specified, will produce messages into a topic named `neo4j` by default.

=== Sink module

Following you'll find a simple docker compose file that allow you to spin-up two Neo4j instances
one configured as `Source` and one as `Sink`, allowing you to share any data from the `Source` to the `Sink`:

* The `Source` is listening at `\http://localhost:8474/browser/` (bolt: `bolt://localhost:8687`)
* The `Sink` is listening at `\http://localhost:7474/browser/` (bolt: `bolt://localhost:7687`) and is configured with the `Schema` strategy.

[source,yml]
----
    environment:
      NEO4J_streams_sink_enabled: "true"
      NEO4J_streams_sink_topic_neo4j:
        "WITH event.value.payload AS payload, event.value.meta AS meta
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'node' AND meta.operation <> 'deleted' and payload.after.labels[0] = 'Question' THEN [1] ELSE [] END |
          MERGE (n:Question{neo_id: toInteger(payload.id)}) ON CREATE
            SET n += payload.after.properties
        )
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'node' AND meta.operation <> 'deleted' and payload.after.labels[0] = 'Answer' THEN [1] ELSE [] END |
          MERGE (n:Answer{neo_id: toInteger(payload.id)}) ON CREATE
            SET n += payload.after.properties
        )
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'node' AND meta.operation <> 'deleted' and payload.after.labels[0] = 'User' THEN [1] ELSE [] END |
          MERGE (n:User{neo_id: toInteger(payload.id)}) ON CREATE
            SET n += payload.after.properties
        )
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'node' AND meta.operation <> 'deleted' and payload.after.labels[0] = 'Tag' THEN [1] ELSE [] END |
          MERGE (n:Tag{neo_id: toInteger(payload.id)}) ON CREATE
            SET n += payload.after.properties
        )
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'relationship' AND meta.operation <> 'deleted' and payload.label = 'ANSWERS' THEN [1] ELSE [] END |
          MERGE (s:Answer{neo_id: toInteger(payload.start.id)})
          MERGE (e:Question{neo_id: toInteger(payload.end.id)})
          CREATE (s)-[:ANSWERS{neo_id: toInteger(payload.id)}]->(e)
        )
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'relationship' AND meta.operation <> 'deleted' and payload.label = 'TAGGED' THEN [1] ELSE [] END |
          MERGE (s:Question{neo_id: toInteger(payload.start.id)})
          MERGE (e:Tag{neo_id: toInteger(payload.end.id)})
          CREATE (s)-[:TAGGED{neo_id: toInteger(payload.id)}]->(e)
        )
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'relationship' AND meta.operation <> 'deleted' and payload.label = 'PROVIDED' THEN [1] ELSE [] END |
          MERGE (s:User{neo_id: toInteger(payload.start.id)})
          MERGE (e:Answer{neo_id: toInteger(payload.end.id)})
          CREATE (s)-[:PROVIDED{neo_id: toInteger(payload.id)}]->(e)
        )
        FOREACH (ignoreMe IN CASE WHEN payload.type = 'relationship' AND meta.operation <> 'deleted' and payload.label = 'ASKED' THEN [1] ELSE [] END |
          MERGE (s:User{neo_id: toInteger(payload.start.id)})
          MERGE (e:Question{neo_id: toInteger(payload.end.id)})
          CREATE (s)-[:ASKED{neo_id: toInteger(payload.id)}]->(e)
        )"
----

==== Launch it locally

In the following example we will use the Neo4j Streams plugin in combination with the APOC procedures (link:https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/latest[download from here])
in order to download some data from Stackoverflow, store them into the Neo4j `Source` instance and replicate these dataset into the `Sink` via the Neo4j Streams plugin.

[source,yml]
----
include::ROOT:partial$docker-data/docker-compose-source-sink.yml[]
----

===== Prerequisites

* Install the APOC into `./neo4j/plugins`.
* Install the Neo4j Streams plugin into `./neo4j/plugins` and `./neo4j/plugins-sink`

===== Import the data

Let's go to two instances in order to create the constraints on both sides:

[source,cypher]
----
// enable the multi-statement execution: https://stackoverflow.com/questions/21778435/multiple-unrelated-queries-in-neo4j-cypher?answertab=votes#tab-top
CREATE CONSTRAINT ON (u:User) ASSERT u.id IS UNIQUE;
CREATE CONSTRAINT ON (a:Answer) ASSERT a.id IS UNIQUE;
CREATE CONSTRAINT ON (t:Tag) ASSERT t.name IS UNIQUE;
CREATE CONSTRAINT ON (q:Question) ASSERT q.id IS UNIQUE;
----

please take a look at the property inside the compose file:

[source,ini]
----
NEO4J_streams_source_schema_polling_interval: 10000
----

this means that every 10 seconds the Streams plugin polls the DB in order to retrieve schema changes and store them.
So after you created the indexes you need almost to wait 10 seconds before the next step.

Now lets go to the `Source` and, in order to import the Stackoverflow dataset, execute the following query:

[source,cypher]
----
UNWIND range(1, 1) as page
CALL apoc.load.json("https://api.stackexchange.com/2.2/questions?pagesize=100&order=desc&sort=creation&tagged=neo4j&site=stackoverflow&page=" + page) YIELD value
UNWIND value.items AS event
MERGE (question:Question {id:event.question_id}) ON CREATE
  SET question.title = event.title, question.share_link = event.share_link, question.favorite_count = event.favorite_count

FOREACH (ignoreMe in CASE WHEN exists(event.accepted_answer_id) THEN [1] ELSE [] END | MERGE (question)<-[:ANSWERS]-(answer:Answer{id: event.accepted_answer_id}))

WITH * WHERE NOT event.owner.user_id IS NULL
MERGE (owner:User {id:event.owner.user_id}) ON CREATE SET owner.display_name = event.owner.display_name
MERGE (owner)-[:ASKED]->(question)
----

Once the import process has finished to be sure that the data is correctly replicated into the `Sink` execute this query
both in `Source` and `Sink` and compare the results:

[source,cypher]
----
MATCH (n)
RETURN
DISTINCT labels(n),
count(*) AS NumofNodes,
avg(size(keys(n))) AS AvgNumOfPropPerNode,
min(size(keys(n))) AS MinNumPropPerNode,
max(size(keys(n))) AS MaxNumPropPerNode,
avg(size((n)-[]-())) AS AvgNumOfRelationships,
min(size((n)-[]-())) AS MinNumOfRelationships,
max(size((n)-[]-())) AS MaxNumOfRelationships
order by NumofNodes desc
----

You can also launch a Kafka Consumer that subscribes the topic `neo4j` by executing this command:

[source,bash]
----
docker exec broker kafka-console-consumer --bootstrap-server broker:9093 --topic neo4j --from-beginning
----

You'll see something like this:

[source,json]
----

{"meta":{"timestamp":1571403896987,"username":"neo4j","txId":34,"txEventId":330,"txEventsCount":352,"operation":"created","source":{"hostname":"neo4j-source"}},"payload":{"id":"94","start":{"id":"186","labels":["User"],"ids":{"id":286795}},"end":{"id":"59","labels":["Question"],"ids":{"id":58303891}},"before":null,"after":{"properties":{}},"label":"ASKED","type":"relationship"},"schema":{"properties":{},"constraints":[]}}

{"meta":{"timestamp":1571403896987,"username":"neo4j","txId":34,"txEventId":331,"txEventsCount":352,"operation":"created","source":{"hostname":"neo4j-source"}},"payload":{"id":"34","start":{"id":"134","labels":["Answer"],"ids":{"id":58180296}},"end":{"id":"99","labels":["Question"],"ids":{"id":58169215}},"before":null,"after":{"properties":{}},"label":"ANSWERS","type":"relationship"},"schema":{"properties":{},"constraints":[]}}


----

[[docker_kafka_connect]]
== Kafka Connect Neo4j Connector

Inside the directory `/kafka-connect-neo4j/docker` you'll find a compose file that allows you to start the whole testing environment:

.docker-compose.yml
[source,yaml]
----
include::ROOT:partial$docker-data/kafka-connect-docker-compose.yml[]
----

include::ROOT:partial$docker-data/readme.adoc[]

[[docker_streams_cluster]]
== Neo4j Streams with Neo4j Cluster and Kafka Cluster

Here we provide a docker-compose file to quickstart with an environment composed by a 3-nodes Neo4j Causal Cluster
(with Streams plugin configured in Sink mode) and a 3-nodes Kafka Cluster.

[source,yaml]
----
include:docker-data/docker-compose-cluster.yml[]
----

What you need to do is just:

* Download the latest Neo4j Streams plugin version from here: https://github.com/neo4j-contrib/neo4j-streams/releases/tag/4.0.1

* Be sure to create the volume folders (into the same folder where the docker-compose file is) `/neo4j-cluster-40/core1/plugins`,
`/neo4j-cluster-40/core2/plugins`, `/neo4j-cluster-40/core3/plugins`, `/neo4j-cluster-40/read1/plugins` and be sure to put the
`neo4j-streams-4.0.1.jar` into those folders.

* Run `docker-compose up -d`

* Connect to Neo4j `core1` instance from the web browser: `localhost:7474`

** Login using the credentials provided in the docker-compose file

** Create a new database (the one where Neo4j Streams Sink is listening), running the following 2 commands from the Neo4j
Browser

*** `:use system`

*** `CREATE DATABASE dbtest`

* Once all the containers are up and running, open a terminal window and connect to Kafka `broker-1`, in order to send
a JSON event using a `kafka-console-producer`. Follow the steps below:

** `docker exec -it broker-1 /bin/bash`

** `kafka-console-producer --broker-list broker-1:29092 --topic mytopic`

** paste the following JSON event into kafka-console-producer:
+
`{"id": 1, "name": "Mauro", "surname": "Roiter"}`.

Here an output example of the last steps:

[source, bash]
----
$ docker exec -it broker-1 /bin/bash
root@broker-1:/# kafka-console-producer --broker-list broker-1:29092 --topic mytopic
>{"id": 1, "name": "Mauro", "surname": "Roiter"}
----

Now if you come back to Neo4j browser, you will see the created node into the respective database `dbtest`.

image::ROOT:docker_streams_cluster_example.png[title="Streams Sink plugin into Neo4j+Kafka cluster environment", align="center"]

You will se the same results in the other Neo4j instances too.

[NOTE]
====
In this example we've used the Neo4j Enterprise docker image because the "CREATE DATABASE" feature is available only into
Enterprise Edition
====
